{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-20T14:12:42.932553Z","iopub.execute_input":"2021-11-20T14:12:42.932846Z","iopub.status.idle":"2021-11-20T14:12:42.937877Z","shell.execute_reply.started":"2021-11-20T14:12:42.932815Z","shell.execute_reply":"2021-11-20T14:12:42.937175Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install timm","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:19.457346Z","iopub.execute_input":"2021-11-20T15:10:19.457744Z","iopub.status.idle":"2021-11-20T15:10:29.352527Z","shell.execute_reply.started":"2021-11-20T15:10:19.457652Z","shell.execute_reply":"2021-11-20T15:10:29.351713Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\n\nimport timm\n\nimport gc\nimport os\nimport time\nimport random\nfrom datetime import datetime\nimport shutil\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection, metrics\nfrom shutil import copyfile","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:22:47.500552Z","iopub.execute_input":"2021-11-20T15:22:47.501201Z","iopub.status.idle":"2021-11-20T15:22:47.507049Z","shell.execute_reply.started":"2021-11-20T15:22:47.501147Z","shell.execute_reply":"2021-11-20T15:22:47.506295Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(111)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:39.103766Z","iopub.execute_input":"2021-11-20T15:10:39.104033Z","iopub.status.idle":"2021-11-20T15:10:39.113358Z","shell.execute_reply.started":"2021-11-20T15:10:39.103997Z","shell.execute_reply":"2021-11-20T15:10:39.112625Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# general global variables\nDATA_PATH = \"/kaggle/input/pollen-dataset/Pollen_data\"\nIMAGES_PATH = os.path.join(DATA_PATH, \"images\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:42.241236Z","iopub.execute_input":"2021-11-20T15:10:42.241980Z","iopub.status.idle":"2021-11-20T15:10:42.246828Z","shell.execute_reply.started":"2021-11-20T15:10:42.241931Z","shell.execute_reply":"2021-11-20T15:10:42.245831Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(DATA_PATH, \"data.csv\"))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:43.164473Z","iopub.execute_input":"2021-11-20T15:10:43.165264Z","iopub.status.idle":"2021-11-20T15:10:43.219091Z","shell.execute_reply.started":"2021-11-20T15:10:43.165226Z","shell.execute_reply":"2021-11-20T15:10:43.218433Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:44.074104Z","iopub.execute_input":"2021-11-20T15:10:44.074933Z","iopub.status.idle":"2021-11-20T15:10:44.100210Z","shell.execute_reply.started":"2021-11-20T15:10:44.074886Z","shell.execute_reply":"2021-11-20T15:10:44.099475Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:44.368671Z","iopub.execute_input":"2021-11-20T15:10:44.369057Z","iopub.status.idle":"2021-11-20T15:10:44.600924Z","shell.execute_reply.started":"2021-11-20T15:10:44.369023Z","shell.execute_reply":"2021-11-20T15:10:44.600263Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Here for splitting the data into train, test and validation. We will using train_test_split from sklearn\n\n# First divide the data into train data (80%) and remaining data(20%)\n# Second divide the remaining data into validation (10%) and test data(10%)\ntrain_df, remaining_df = model_selection.train_test_split(df, test_size=0.2, random_state=42, stratify=df.label.values)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:47.011838Z","iopub.execute_input":"2021-11-20T15:10:47.012231Z","iopub.status.idle":"2021-11-20T15:10:47.029935Z","shell.execute_reply.started":"2021-11-20T15:10:47.012196Z","shell.execute_reply":"2021-11-20T15:10:47.029275Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Training data distribution\ntrain_df.label.value_counts().plot(kind=\"bar\")\nplt.title(\"Training data distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:47.345162Z","iopub.execute_input":"2021-11-20T15:10:47.345498Z","iopub.status.idle":"2021-11-20T15:10:47.655133Z","shell.execute_reply.started":"2021-11-20T15:10:47.345459Z","shell.execute_reply":"2021-11-20T15:10:47.654255Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:47.659762Z","iopub.execute_input":"2021-11-20T15:10:47.660045Z","iopub.status.idle":"2021-11-20T15:10:47.673714Z","shell.execute_reply.started":"2021-11-20T15:10:47.660012Z","shell.execute_reply":"2021-11-20T15:10:47.672849Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"remaining_df.label.value_counts().plot(kind=\"bar\")\nplt.title(\"Except training data distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:47.902527Z","iopub.execute_input":"2021-11-20T15:10:47.903075Z","iopub.status.idle":"2021-11-20T15:10:48.112937Z","shell.execute_reply.started":"2021-11-20T15:10:47.903038Z","shell.execute_reply":"2021-11-20T15:10:48.112198Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Dividing remaining data into validation and test set\nvalid_df, test_df = model_selection.train_test_split(remaining_df, test_size=0.5, random_state=42, stratify=remaining_df.label.values)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:48.175501Z","iopub.execute_input":"2021-11-20T15:10:48.176039Z","iopub.status.idle":"2021-11-20T15:10:48.186411Z","shell.execute_reply.started":"2021-11-20T15:10:48.176003Z","shell.execute_reply":"2021-11-20T15:10:48.185605Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"valid_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:50.247686Z","iopub.execute_input":"2021-11-20T15:10:50.248364Z","iopub.status.idle":"2021-11-20T15:10:50.255558Z","shell.execute_reply.started":"2021-11-20T15:10:50.248328Z","shell.execute_reply":"2021-11-20T15:10:50.254916Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"valid_df.label.value_counts().plot(kind=\"bar\")\nplt.title(\"Validation data distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:50.411354Z","iopub.execute_input":"2021-11-20T15:10:50.412938Z","iopub.status.idle":"2021-11-20T15:10:50.619476Z","shell.execute_reply.started":"2021-11-20T15:10:50.412894Z","shell.execute_reply":"2021-11-20T15:10:50.618793Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"test_df.label.value_counts().plot(kind=\"bar\")\nplt.title(\"Test data distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:50.622025Z","iopub.execute_input":"2021-11-20T15:10:50.622275Z","iopub.status.idle":"2021-11-20T15:10:50.822896Z","shell.execute_reply.started":"2021-11-20T15:10:50.622243Z","shell.execute_reply":"2021-11-20T15:10:50.822210Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nclass PollenDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Helper Class to create the pytorch dataset\n    \"\"\"\n\n    def __init__(self, df, data_path=DATA_PATH, transforms=None):\n        super().__init__()\n        self.df_data = df.values\n        self.data_path = data_path\n        self.transforms = transforms\n        self.data_dir = \"images\"\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, index):\n        img_name, label = self.df_data[index]\n        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        label = np.asarray(label, dtype='int64')\n\n        if self.transforms is not None:\n            image = self.transforms(img)\n\n        return image, torch.from_numpy(label)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:54.267287Z","iopub.execute_input":"2021-11-20T15:10:54.267544Z","iopub.status.idle":"2021-11-20T15:10:54.275775Z","shell.execute_reply.started":"2021-11-20T15:10:54.267516Z","shell.execute_reply":"2021-11-20T15:10:54.274914Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\nBATCH_SIZE = 16","metadata":{"execution":{"iopub.status.busy":"2021-11-20T15:10:56.477626Z","iopub.execute_input":"2021-11-20T15:10:56.478366Z","iopub.status.idle":"2021-11-20T15:10:56.482373Z","shell.execute_reply.started":"2021-11-20T15:10:56.478330Z","shell.execute_reply":"2021-11-20T15:10:56.481511Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# create image augmentations\n\ntransforms_train = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.RandomVerticalFlip(p=0.3),\n        transforms.RandomHorizontalFlip(p=0.3),\n        transforms.RandomResizedCrop(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)\n\ntransforms_valid = transforms.Compose(\n    [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:18.368390Z","iopub.execute_input":"2021-11-20T16:09:18.368644Z","iopub.status.idle":"2021-11-20T16:09:18.376012Z","shell.execute_reply.started":"2021-11-20T16:09:18.368613Z","shell.execute_reply":"2021-11-20T16:09:18.375286Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"# Train and validation dataset with transformations\ntrain_dataset = PollenDataset(train_df, transforms=transforms_train)\nvalid_dataset = PollenDataset(valid_df, transforms=transforms_valid)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:19.239333Z","iopub.execute_input":"2021-11-20T16:09:19.239949Z","iopub.status.idle":"2021-11-20T16:09:19.247509Z","shell.execute_reply.started":"2021-11-20T16:09:19.239910Z","shell.execute_reply":"2021-11-20T16:09:19.246446Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# Train and validation loader \ntrain_loader = DataLoader(\n     dataset=train_dataset,\n     batch_size=BATCH_SIZE,\n     shuffle = True\n     )\nvalid_loader = DataLoader(\n    dataset=valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:21.559858Z","iopub.execute_input":"2021-11-20T16:09:21.560427Z","iopub.status.idle":"2021-11-20T16:09:21.567491Z","shell.execute_reply.started":"2021-11-20T16:09:21.560389Z","shell.execute_reply":"2021-11-20T16:09:21.566652Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"train_sample = iter(train_loader)\ndata, label = train_sample.next()\nprint(data.shape)\nprint(data.max())\nprint(data.min())","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:21.717467Z","iopub.execute_input":"2021-11-20T16:09:21.717685Z","iopub.status.idle":"2021-11-20T16:09:21.794819Z","shell.execute_reply.started":"2021-11-20T16:09:21.717658Z","shell.execute_reply":"2021-11-20T16:09:21.794106Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"data.dtype","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:24.175759Z","iopub.execute_input":"2021-11-20T16:09:24.176412Z","iopub.status.idle":"2021-11-20T16:09:24.183854Z","shell.execute_reply.started":"2021-11-20T16:09:24.176375Z","shell.execute_reply":"2021-11-20T16:09:24.181870Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"print(\"Available Vision Transformer Models: \")\ntimm.list_models(\"vit*\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:24.353631Z","iopub.execute_input":"2021-11-20T16:09:24.354257Z","iopub.status.idle":"2021-11-20T16:09:24.361691Z","shell.execute_reply.started":"2021-11-20T16:09:24.354217Z","shell.execute_reply":"2021-11-20T16:09:24.360718Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"markdown","source":"### Will be using _patch16_224_ and _patch32_224_ vit models for training and testing","metadata":{}},{"cell_type":"code","source":"# VIT model with patch16\nclass ViTBase16(nn.Module):\n    def __init__(self, n_classes, pretrained=False):\n\n        super(ViTBase16, self).__init__()\n        \n        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained)\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes) # Classification head\n\n    def forward(self, x):\n        x = self.model(x)\n        return x  \n\n# VIT model with patch32\nclass ViTBase32(nn.Module):\n    def __init__(self, n_classes, pretrained=False):\n\n        super(ViTBase32, self).__init__()\n        \n        self.model = timm.create_model(\"vit_base_patch32_224\", pretrained)\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes) # Classification head\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:28.020015Z","iopub.execute_input":"2021-11-20T16:09:28.020286Z","iopub.status.idle":"2021-11-20T16:09:28.031089Z","shell.execute_reply.started":"2021-11-20T16:09:28.020255Z","shell.execute_reply":"2021-11-20T16:09:28.030029Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"model = ViTBase16(n_classes=4, pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:40.449957Z","iopub.execute_input":"2021-11-20T16:09:40.450713Z","iopub.status.idle":"2021-11-20T16:09:45.217188Z","shell.execute_reply.started":"2021-11-20T16:09:40.450663Z","shell.execute_reply":"2021-11-20T16:09:45.216361Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:46.684813Z","iopub.execute_input":"2021-11-20T16:09:46.685625Z","iopub.status.idle":"2021-11-20T16:09:46.690665Z","shell.execute_reply.started":"2021-11-20T16:09:46.685584Z","shell.execute_reply":"2021-11-20T16:09:46.689690Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:47.739382Z","iopub.execute_input":"2021-11-20T16:09:47.740131Z","iopub.status.idle":"2021-11-20T16:09:56.570562Z","shell.execute_reply.started":"2021-11-20T16:09:47.740081Z","shell.execute_reply":"2021-11-20T16:09:56.569707Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nmodel = model.to(device)\nsummary(model, (3, 224, 224))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:56.573647Z","iopub.execute_input":"2021-11-20T16:09:56.573945Z","iopub.status.idle":"2021-11-20T16:09:56.742706Z","shell.execute_reply.started":"2021-11-20T16:09:56.573916Z","shell.execute_reply":"2021-11-20T16:09:56.741209Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"total trainable parameters = {} Million\".format(str(round(total_params/1000000))))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:56.744451Z","iopub.execute_input":"2021-11-20T16:09:56.744708Z","iopub.status.idle":"2021-11-20T16:09:56.804934Z","shell.execute_reply.started":"2021-11-20T16:09:56.744671Z","shell.execute_reply":"2021-11-20T16:09:56.804162Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"# Checking the model performance with out any training, on training set, so that we can \n# Conclude Later during that the model is atleast converging later on\nbaseline_train_loss = 0.0\nbaseline_train_accuracy = 0.0\ncriterion = nn.CrossEntropyLoss()\nwith torch.no_grad():\n    model.eval()\n    for data, target in train_loader:\n        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.int64)\n        output = model(data)\n        loss = criterion(output, target)\n        accuracy = (output.argmax(dim=1) == target).float().mean()\n        baseline_train_loss += loss\n        baseline_train_accuracy += accuracy","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:09:59.063371Z","iopub.execute_input":"2021-11-20T16:09:59.063635Z","iopub.status.idle":"2021-11-20T16:10:56.997874Z","shell.execute_reply.started":"2021-11-20T16:09:59.063604Z","shell.execute_reply":"2021-11-20T16:10:56.997085Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"print(\"Total Training batches = {}\".format(len(train_loader)))\nprint(\"Baseline Training Data Loss = {}\".format(baseline_train_loss/len(train_loader)))\nprint(\"Baseline Training Data Accuracy = {} %\".format(100*baseline_train_accuracy/len(train_loader)))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:10:56.999636Z","iopub.execute_input":"2021-11-20T16:10:56.999912Z","iopub.status.idle":"2021-11-20T16:10:57.087099Z","shell.execute_reply.started":"2021-11-20T16:10:56.999876Z","shell.execute_reply":"2021-11-20T16:10:57.086276Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"baseline_valid_loss = 0.0\nbaseline_valid_accuracy = 0.0\nwith torch.no_grad():\n    model.eval()\n    for data, target in valid_loader:\n        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.int64)\n        output = model(data)\n        loss = criterion(output, target)\n        accuracy = (output.argmax(dim=1) == target).float().mean()\n        baseline_valid_loss += loss\n        baseline_valid_accuracy += accuracy","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:10:57.088439Z","iopub.execute_input":"2021-11-20T16:10:57.088777Z","iopub.status.idle":"2021-11-20T16:11:04.240424Z","shell.execute_reply.started":"2021-11-20T16:10:57.088738Z","shell.execute_reply":"2021-11-20T16:11:04.239672Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"print(\"Total Validation batches = {}\".format(len(valid_loader)))\nprint(\"Baseline Validation Data Loss = {}\".format(baseline_valid_loss/len(valid_loader)))\nprint(\"Baseline Validation Data Accuracy = {} %\".format(100*baseline_valid_accuracy/len(valid_loader)))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.242544Z","iopub.execute_input":"2021-11-20T16:11:04.242831Z","iopub.status.idle":"2021-11-20T16:11:04.286418Z","shell.execute_reply.started":"2021-11-20T16:11:04.242777Z","shell.execute_reply":"2021-11-20T16:11:04.285640Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"## Freezing all the layers","metadata":{}},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.287839Z","iopub.execute_input":"2021-11-20T16:11:04.288098Z","iopub.status.idle":"2021-11-20T16:11:04.293139Z","shell.execute_reply.started":"2021-11-20T16:11:04.288063Z","shell.execute_reply":"2021-11-20T16:11:04.292320Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"total trainable parameters = {} Million\".format(str(round(total_params/1000000))))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.294729Z","iopub.execute_input":"2021-11-20T16:11:04.295056Z","iopub.status.idle":"2021-11-20T16:11:04.304254Z","shell.execute_reply.started":"2021-11-20T16:11:04.295017Z","shell.execute_reply":"2021-11-20T16:11:04.303453Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"model.model","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.305276Z","iopub.execute_input":"2021-11-20T16:11:04.305463Z","iopub.status.idle":"2021-11-20T16:11:04.318144Z","shell.execute_reply.started":"2021-11-20T16:11:04.305440Z","shell.execute_reply":"2021-11-20T16:11:04.317373Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"def unfreeze_blocks(model, blocks=[11]): # default unfreeze the 11th block only\n    for i in blocks:\n        for param in model.model.blocks[i].parameters():\n            param.requires_grad = True\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.320863Z","iopub.execute_input":"2021-11-20T16:11:04.321319Z","iopub.status.idle":"2021-11-20T16:11:04.326645Z","shell.execute_reply.started":"2021-11-20T16:11:04.321281Z","shell.execute_reply":"2021-11-20T16:11:04.325832Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"model = unfreeze_blocks(model, [10, 11])","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.327943Z","iopub.execute_input":"2021-11-20T16:11:04.328213Z","iopub.status.idle":"2021-11-20T16:11:04.336417Z","shell.execute_reply.started":"2021-11-20T16:11:04.328175Z","shell.execute_reply":"2021-11-20T16:11:04.335657Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"total trainable parameters = {} Million\".format(str(round(total_params/1000000))))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.339301Z","iopub.execute_input":"2021-11-20T16:11:04.339713Z","iopub.status.idle":"2021-11-20T16:11:04.347732Z","shell.execute_reply.started":"2021-11-20T16:11:04.339676Z","shell.execute_reply":"2021-11-20T16:11:04.346849Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"def unfreeze_head(model):\n    # Final MLP heads of the classifier, unfreezing\n    for param in model.model.norm.parameters():\n        param.requires_grad = True\n\n    for param in model.model.pre_logits.parameters():\n        param.requires_grad = True\n\n    for param in model.model.head.parameters():\n        param.requires_grad = True\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.348971Z","iopub.execute_input":"2021-11-20T16:11:04.349355Z","iopub.status.idle":"2021-11-20T16:11:04.356077Z","shell.execute_reply.started":"2021-11-20T16:11:04.349317Z","shell.execute_reply":"2021-11-20T16:11:04.355100Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"model = unfreeze_head(model)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"total trainable parameters = {} Million\".format(str(round(total_params/1000000))))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.357314Z","iopub.execute_input":"2021-11-20T16:11:04.358137Z","iopub.status.idle":"2021-11-20T16:11:04.369296Z","shell.execute_reply.started":"2021-11-20T16:11:04.358084Z","shell.execute_reply":"2021-11-20T16:11:04.368412Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"#### Unfreezing the head added 4612 extra parameters","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        print(\"Saving the best model !\")\n        shutil.copyfile(filename, 'model_best.pth.tar')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.370502Z","iopub.execute_input":"2021-11-20T16:11:04.370964Z","iopub.status.idle":"2021-11-20T16:11:04.378255Z","shell.execute_reply.started":"2021-11-20T16:11:04.370926Z","shell.execute_reply":"2021-11-20T16:11:04.377530Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"# Initializations \nLR = 0.001\nepochs = 5\ncheck_every = 100\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\ntrain_loss_array = []\ntrain_acc_array = []\nval_loss_array = []\nval_acc_array = []\nbest_acc1 = 0","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:04.379686Z","iopub.execute_input":"2021-11-20T16:11:04.380239Z","iopub.status.idle":"2021-11-20T16:11:04.389601Z","shell.execute_reply.started":"2021-11-20T16:11:04.380197Z","shell.execute_reply":"2021-11-20T16:11:04.388825Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"# Begin training\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    epoch_accuracy = 0.0\n    i = 0\n    for counter, (data, target) in enumerate(train_loader):\n        i += 1\n        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.int64) # load data to device\n        \n        # clear the gradients of all optimizable variables\n        optimizer.zero_grad()\n        # compute outputs by passing input to the model\n        output = model(data)\n        # the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # Calculating accuracy\n        accuracy = (output.argmax(dim=1) == target).float().mean()\n    \n        # update training loss and accuracy\n        epoch_loss += loss\n        epoch_accuracy += accuracy\n        optimizer.step()\n        \n        if i % check_every == 0:\n            # keep track of validation loss\n            valid_loss = 0.0\n            valid_accuracy = 0.0\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_loader:\n                    data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.int64)\n                    output = model(data)\n                    loss = criterion(output, target)\n                    accuracy = (output.argmax(dim=1) == target).float().mean()\n                    # update average validation loss and accuracy\n                    valid_loss += loss\n                    valid_accuracy += accuracy\n            \n            # Score transfer to CPU\n            valid_loss_cpu = valid_loss.cpu().detach().numpy() \n            valid_accuracy_cpu = valid_accuracy.cpu().detach().numpy() \n            epoch_loss_cpu = epoch_loss.cpu().detach().numpy() \n            epoch_accuracy_cpu = epoch_accuracy.cpu().detach().numpy() \n            \n            val_loss_array.append(valid_loss_cpu/len(valid_loader)) \n            val_acc_array.append(valid_accuracy_cpu/len(valid_loader))\n            train_loss_array.append(epoch_loss_cpu/(counter+1)) \n            train_acc_array.append(epoch_accuracy_cpu/(counter+1))\n            print(\"[{} epoch {} batch] Train Loss : {:.3f} \\t Train Accuracy : {:.3f} \\t Valid loss : {:.3f} \\t Valid Accuracy : {:.3f}\".format(epoch+1, \n                                                                                                                                i, \n                                                                                                                                epoch_loss_cpu/(counter+1), \n                                                                                                                                epoch_accuracy_cpu/(counter+1), \n                                                                                                                                valid_loss_cpu / len(valid_loader), \n                                                                                                                                valid_accuracy_cpu / len(valid_loader)))\n            acc1 = valid_accuracy/len(valid_loader)\n            is_best = acc1 > best_acc1\n            best_acc1 = max(acc1, best_acc1)\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n            }, is_best)\n        model.train()\n    \n#     print(\"Epoch : {} Train loss : {} \\t Train Accuracy : {}\".format(epoch+1, epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)))\nprint(\"Finish Training!\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:11:17.431339Z","iopub.execute_input":"2021-11-20T16:11:17.431645Z","iopub.status.idle":"2021-11-20T16:21:16.327121Z","shell.execute_reply.started":"2021-11-20T16:11:17.431611Z","shell.execute_reply":"2021-11-20T16:21:16.325646Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"epoch_array = np.linspace(1, epochs, len(train_loss_array))\nplt.plot(epoch_array, train_loss_array, label = \"Train Loss\")\nplt.plot(epoch_array, val_loss_array, label = \"Validation Loss\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Curve')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:21:23.373499Z","iopub.execute_input":"2021-11-20T16:21:23.373754Z","iopub.status.idle":"2021-11-20T16:21:23.596480Z","shell.execute_reply.started":"2021-11-20T16:21:23.373725Z","shell.execute_reply":"2021-11-20T16:21:23.595833Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"plt.plot(epoch_array, train_acc_array, label = \"Train Accuracy\")\nplt.plot(epoch_array, val_acc_array, label = \"Validation Accuracy\")\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train and Validation Accuracy Curve')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:21:30.617549Z","iopub.execute_input":"2021-11-20T16:21:30.618399Z","iopub.status.idle":"2021-11-20T16:21:30.840284Z","shell.execute_reply.started":"2021-11-20T16:21:30.618352Z","shell.execute_reply":"2021-11-20T16:21:30.839618Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"### Performance measure of best model","metadata":{}},{"cell_type":"code","source":"# Best validation accuracy Vit16_224 Model\nPATH = 'model_best.pth.tar'\nstate = torch.load(PATH)\n\nmodel = ViTBase16(n_classes=4, pretrained=False)\nmodel.load_state_dict(state['state_dict'])\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using {}\".format(device))\nmodel = model.to(device=device)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:21:35.774243Z","iopub.execute_input":"2021-11-20T16:21:35.774728Z","iopub.status.idle":"2021-11-20T16:21:37.429665Z","shell.execute_reply.started":"2021-11-20T16:21:35.774676Z","shell.execute_reply":"2021-11-20T16:21:37.428860Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"print(\"######### Generating confusion Matrix on Validation Set ########\")\nprediction = []\nground_truth = []\nwith torch.no_grad():\n    model.eval()\n    for data, target in valid_loader:\n        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.int64)\n        output = model(data)\n        prediction.extend(output.argmax(dim=1))\n        ground_truth.extend(target)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:22:03.508997Z","iopub.execute_input":"2021-11-20T16:22:03.509318Z","iopub.status.idle":"2021-11-20T16:22:10.689509Z","shell.execute_reply.started":"2021-11-20T16:22:03.509281Z","shell.execute_reply":"2021-11-20T16:22:10.688720Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"prediction_array = [int(i.cpu().detach().numpy()) for i in prediction]\nground_truth_array = [int(i.cpu().detach().numpy()) for i in ground_truth]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:22:10.691986Z","iopub.execute_input":"2021-11-20T16:22:10.693191Z","iopub.status.idle":"2021-11-20T16:22:10.779091Z","shell.execute_reply.started":"2021-11-20T16:22:10.693127Z","shell.execute_reply":"2021-11-20T16:22:10.778458Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(ground_truth_array, prediction_array))\nprint(confusion_matrix(ground_truth_array, prediction_array))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:22:12.970841Z","iopub.execute_input":"2021-11-20T16:22:12.971541Z","iopub.status.idle":"2021-11-20T16:22:12.992495Z","shell.execute_reply.started":"2021-11-20T16:22:12.971502Z","shell.execute_reply":"2021-11-20T16:22:12.991385Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ncm = confusion_matrix(ground_truth_array, prediction_array)\ncmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(10,10))\nplt.title(\"Confusion Matrix of Patch16 model on validation set\")\nsns.heatmap(cmn, annot=True, fmt='.2f')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show(block=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T16:30:47.434174Z","iopub.execute_input":"2021-11-20T16:30:47.434435Z","iopub.status.idle":"2021-11-20T16:30:47.715277Z","shell.execute_reply.started":"2021-11-20T16:30:47.434407Z","shell.execute_reply":"2021-11-20T16:30:47.714586Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}